The concept of implementation can be divided into the used packages (\emph{Stable Baselines3, Gym Pybullet Drones, ...}), the scripts, the environment classes and different support classes (\cref{fig:concept}).\\
Stable Baselines3 is used as implementation of the PPO algorithm (\cref{alg:ppo}) in order to achieve the tasks. Gym Pybullet Drones has already some environments and is the foundation of the simulation. It already provides a suitable step function and a well designed physics engine.\\
The scripts (\cref{sec:scripts} , lila) are used to either learn the agent on a given environment or evaluate it.\\
The environment classes(\cref{sec:env}, green) inherit from a \emph{Gym Environment} and models a MDP. By modelling this MDP precisely, it is defined what is learned later by the intelligent agent. It uses the wind class in order to model a harsh environment.
In addition, there are a couple of evaluation tools (\cref{sec:tools}, red), that supports the implemented classes.\\
\newline
The whole concept is implemented in Python3.x with the use of a \emph{conda environment}.

\begin{figure}[htp]
	\centering
	\includegraphics[width= \linewidth]{figures/concept.png}
	\caption{Concept of the implemented software: the different tools(red), scripts(violet) that are used in order to learn the intelligent Agent robust flight control with the use of RL.}
	\label{fig:concept}
\end{figure}
\newpage

\section{Environment Classes} \label{sec:env}



\subsection{WindSingleAgentAviary Environment Class}

\newpage

\subsubsection{Wind Class}

\newpage

\subsubsection{Modes}\label{sec:modes}

\newpage

\subsubsection{Observation Space \& State Space}
The state space $S$ defines the state vector of the drone in the environment and possesses a dimensionality of $23$. This state is only used inside the evironment and consists of the current position $x_p, y_p, z_p$ in each axis, the roll, pitch and yaw angles $\Theta_p, \phi_p, \psi_p$ as well as represented as quaterion $q$, the velocities $\dot{x_p}, \dot{y_p}, \dot{z_p}$, the angular velocities $dot{\Theta_p}, \dot{\phi_p}, \dot{\psi_p}$, the goal position $x_g, y_g, z_g$ and the last clipped action $a_t$. With the use of this drone state space the observations are calculated.\\
\newline
The observation space $\sigma$ is a subset of the state space $S$ with the dimensionality of 15. The observation space is implemented as a \emph{spaces box} of type \emph{float32} wich are mainly ranged within $[-1, 1]$ with the exception of the z coordinate of the position $z_p$ and goal $z_g$. Because there is floor defined as a plain at the height of $0$, which inherits a collision body, the drone is not able to reach a negative z coordinate. As a consequence, these are ranged to $[0,1]$.
\newline
\begin{align}
	\sigma_t &= (x_p,y_p,z_p, \Theta_p, \phi_p, \psi_p, \dot{x_p}, \dot{y_p}, \dot{z_p}, \dot{\Theta_p}, \dot{\phi_p}, \dot{\psi_p}, x_g, y_g, z_g) \label{eq:obs}
\end{align}
\newline
Each observation $\sigma_t$ consists of the current position $x_p, y_p, z_p$ in each axis, the roll, pitch and yaw angles $\Theta_p, \phi_p, \psi_p$, the velocities $\dot{x_p}, \dot{y_p}, \dot{z_p}$, the angular velocities $dot{\Theta_p}, \dot{\phi_p}, \dot{\psi_p}$ and the goal position $x_g, y_g, z_g$ (\cref{eq:obs}). These observations are given to the NN in order to approximate the optimal action $a$ that satefies the defined RL problem.\\
\newline
Like previously mentioned, all observations are ranged in order to prohibit inputs of different magnitude that could disrupt the learning process. This is done with the use of the method \emph{\_clipAndNormalizeState} which gets the current state $s_t$ and normalizes it to the defined range (\cref{eq:clipnorm}). First, it clips it to predifined values $v$ and then normalizes it by deviding with the matching predefined value $v_i, i\in[0,11]$. If wanted, a warning can be printed each time a state parameter has to be clipped. By clipping x and y to a value in $[-20,20]$ there is a predifed limit of maximal distance, in which there is a reasonable option to learn robust flight. Analogue the z component is clipped to $[-10,10]$, roll and pitch to $[-\pi, \pi]$, the translation velocities to $[-3, 3]$ in x,y and to $[-2,2]$ in z direction. 
\newline
\begin{align}
	\sigma_t \leftarrow clip(s_t) / v \label{eq:clipnorm}
\end{align}
\newline

\newpage

\subsubsection{Action Space}
The WindSingleAgentAviary Environment possesses three different type of action spaces, that are processed in different ways to the \emph{rpm (rotation per minute)} of the four motors (\cref{tab:act}). Nevertheless, all action types are continuous and are ranged in $[-1, 1]$.\\
\newline
\emph{one\_d\_rpm} is a one dimensional action space. The chosen action $a$ is processed to range of $\alpha$ about the \emph{hover\_rpm} to a 4-tupel of rpms. The hover\_rpm is defined as the rpm that corresponds to hovering (\cref{sec:hover}). The 4-tupel is then forwarded to the motors. As a consequence of this limitation, the drone can only perform hovering, rising and falling movements and can not influence its $x,y$ position or $\Theta, \phi, \psi$. Also, the translational speed $v_z$ is limited by the size of $\alpha$.\\
\newline
\emph{rpm} is a 4 dimensional action space. The actions are processed within a range of $\alpha$ about the hover\_rpm to a 4-tupel of rpms. The drone is not limited in any dimensionality, but the task increases in complexity. Due to \cref{form:quad}, \cref{form:quad2}, \cref{form:quad3}, \cref{form:quad4} even a small difference in rpms can lead to an unstable flight or even a crash, because the roll or pitch angle is to high. Also, all translational an rotational speeds are limited by the size of $\alpha$. Because of the higher complexity, it is expected, that the training takes noticeable more time.\\
\newline
\emph{vel} is a top level, 4-dimensional action space. The action consists of a velocity vector $v = (a_0, a_1, a_2)$ and its size $a_3$.  Since it is a top level action space, the actions are not corresponding directly to the rpms, but a pid controller (\cref{sec:pid}) is used in order to control the rpms. The basic pid controller is part of Gym Pybullet Drones \cite{panerati2021learning} and must be tuned for the used quadrocopter. It mainly receives the state $S$ of the drone, as well as the targeted velocity, which is calculated with the use of the actions. Therefore, $a_3$ is multiplicated with speed limit of the drone in order to derange the action. Also, the velocity vector $v$ is normalized to a length of $1$.\\
The drone is not limited in any dimensionality and the task is less complex then setting rpm directly. Stability of the flight is now mainly controlled by the pid controller, so it is bounded by the typical pid constraints in harsh environments and not adaptable.
\begin{table}
	\centering
	\caption{The different ActionTypes with the corresponding dimensionality of the action, its range and how it is processed.}\label{tab:act}
	\begin{tabular}{c|c|c|c}
		ActionType & dim & range & processing\\
		\hline
		\emph{one\_d\_rpm} & $|a| = 1$ & $a_i \in [-1, 1]$ & $rpm = (hover\_rpm \cdot (1 + \alpha \cdot  a)) \cdot (1, 1, 1, 1)$ \\
		\emph{rpm} & $|a| = 4$ & $a_i \in [-1, 1]$ & $rpm =  hover\_rpm \cdot (1 + \alpha \cdot  a)$ \\
		\emph{vel} & $|a| = 4$ & $a_i \in [-1,1]$ & $rpm = pid(S, vel= limit \cdot  |a_3| \cdot \frac{(a_0,a_1,a_2)}{|(a_0,a_1,a_2)|})$
	\end{tabular}
\end{table}


\newpage

\subsubsection{Reward}
\begin{figure}
	\centering
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[width=\linewidth]{figures/rew3d.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[width=\linewidth]{figures/rewXY.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.3\linewidth}
		\includegraphics[width=\linewidth]{figures/rewXZ.png}
		\caption{}
	\end{subfigure}
	\caption{Visualization of the used reward function with the goal ($0,0,0.5$) and a colour scale for different positions in space.}
\end{figure}

\begin{align}
	r_t &= e^{-0.6 \cdot |goal - pos|} - 1 \label{eq:rew}
\end{align}

\newpage

\begin{figure}
	\centering
	\includegraphics[width= 0.5\linewidth]{figures/reward.png}
	\caption{Visualization of the reward functions over the total distance[m]}
\end{figure}

\subsubsection{Constraints}
\begin{align}
	z_{p_t} \leq z_{min} \land \frac{t}{f} \geq 0.1 \to r_t = e^{-0.6 \cdot |goal - pos|} - 1 -200 \land done
\end{align}

\newpage

\subsubsection{Optimal Rewards}
Based on the modes (\cref{sec:modes}) different optimal rewards can be defined, that are helpful in order to determine how good the learned desicion making is.\\
\newline
\emph{Mode 0} is due to its fixed goal position $g$ rather easy to define:
\begin{align}
R_{opt} &= \sum_{t=0}^{f \cdot episode\_len} r_t \label{eq:oprew}\\
d_0 &= 0.5\\
d_{t+1} &= d_t - |v_{opt}| \cdot f \label{eq:dt1}\\ 
r_{t} &= e^{-0.6 \cdot (d_{t+1} )} - 1 \label{eq:rew2}
\end{align}
The distance at start $d_0$ is always 0.5. An optimal decision of the agent corresponds to an optimal velocity vector $v_{opt}$ that minimizes the distance to the goal with the given bounds of the maximum speed at the $\alpha$ of the action processing. The reward (\cref{eq:rew}) can then be reduced to \cref{eq:rew2} for every control step and summed up to the total optimal reward $R_{opt}$ (\cref{eq:oprew}).\\
\newline
Since \emph{mode 1} does not posess a fixed goal position, only a expected optimal reward $\mathbb{E}(R_{opt})$ can be defined. Therefore, an expected starting distance $\mathbb{E}(d_0)$ has to be used:
\begin{align}
	\mathbb{E}(d_0) &= \sqrt{\mathbb{E}(d_x)^2 + \mathbb{E}(d_y)^2 + \mathbb{E}(d_z)^2} \\
	\mathbb{E}(d_i) &= \int_{b_{min}}^{b_{max}} |i| di \\
	&= 
	\left\{
	\begin{array}{ll}
		 \int_{b_{min}}^{b_{max}} i di & \mbox{if } 0 \leq b_{min} \leq b_{max} \\
		 \int_{b_{min}}^{b_{max}} -i di & \mbox{if } b_{min} \leq b_{max} \leq 0\\
		\int_{b_{min}}^{0} -i di + \int_{0}^{b_{max}} i di  & \mbox{if } b_{min} \leq 0 \leq b_{max}
	\end{array}
	\right. \\
	&=
	\left\{
	\begin{array}{ll}
		\frac{1}{2} \cdot (b_{max}^2 - b_{min}^2) \enspace \enspace \enspace \enspace  & \mbox{if } 0 \leq b_{min} \leq b_{max} \\
		\frac{1}{2} \cdot (b_{min}^2 - b_{max}^2)  & \mbox{if } b_{min} \leq b_{max} \leq 0\\
		\frac{1}{2} \cdot (b_{min}^2 + b_{max}^2) & \mbox{if } b_{min} \leq 0 \leq b_{max}
	\end{array}
	\right.
	\label{eq:expd}
	%\int_{b_{min}}^{0} -i di + \int_{0}^{b_{max}} i di = [-\frac{1}{2}i^2]_{b_{min}}^{0} + [\frac{1}{2}i^2]_0^{b_{max}}\\
	%&= -(- \frac{1}{2} b_{min}^2) + \frac{1}{2} b_{max}^2 = \frac{1}{2} \cdot (b_{min}^2 + b_{max}^2)
\end{align}
The expected distance in each axis $\mathbb{E}(d_i)$ mostly depend on the defined bounds of the goal.\\
\newline
All other modes possess a wind class that trys to disrupt the agent. Since there are random wind fields and the corresponding force is not noramlly distributed, the optimal reward can just estimated downwords. Therefore, the used maximum wind force $W$ is used in order to approximate the maximum of disruptive distance and use it in order to update \cref{eq:dt1}.
\begin{align}
	d_{t+1} &= d_t - | v_{opt} | \cdot f + \frac{W}{m} \cdot f
\end{align}
\newpage

\subsection{WindSingleAgentPathfollowingAviary Environment Class}


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scripts \& Evaluation Tools} \label{sec:scripts}


%\subsection{TestInstall Script}

\subsection{Learning Script}
\begin{algorithm}
	\caption{Learning Script}
	\label{alg:learn}
	 Parse the arguments to the Script
	 
	 Check the parsed arguments on contradiction and raise ParsingError if needed
	 
	 Create training environment with the parsed parameters
	 
	 Define the size of the actor and critic policy network
	 
	 Create or load the model based on the parsed load argument
	 
	 Create evaluation environment with the parsed parameters
	 
	 Define evaluation callbacks
	 
	 Learn the model with the parsed amount of time steps
	 
	 Save the model with the parsed preferred name
	 
	 
\end{algorithm}

\newpage

\subsection{Evaluation Script}
\begin{algorithm}
	\caption{Evaluation Script}
	\label{alg:eval}
	Parse the arguments to the Script
	
	Check the parsed arguments on contradiction and raise ParsingError if needed
	
	Create evaluation environment with the parsed parameters
	
	Define the size of the actor and critic policy network
	
	Load the model
	
	Instantiate an EvalWriter and evaluate the model
	
	\If{gui parameter was parsed as true}{
		Instantiate a test environement, logger and a pathplotter
		
		Reset environment and safe an observation $\sigma$
		
		
		Safe current time $t$
		
		\Repeat{done or time is over}{
		Get action $a$ and the observation $\sigma$
		
		Execute a step in the test environment and receive an observation $\sigma$, a reward $r$, a done value and a info
		
		Add current pose to pathplotter
		
		Log current drone state
		
		Sync simulation time $t_sim$ to the real time $t$}
		Close test environment, show the plotted path and the values.
	}


\end{algorithm}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation Tools} \label{sec:tools}

\subsubsection{EvalWriter Class}

\newpage

\subsubsection{PathPlotter Class}
