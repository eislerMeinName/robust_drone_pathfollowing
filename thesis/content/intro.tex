In the last decade, an increase of popularity in the area of automated flight
control could be observed. While a wide range of companies sell classic
drones like Quadrocopter, these are mostly remotely piloted and used for
hobby purposes.
However, automated flight control can simplify many jobs and in total a
lot of work can be accomplished more efficiently.
In addition, autonomous UAVs can even be used in a wider range of application,
because a pilot is not needed.
Therefore, autonomous UAVs can operate in misanthropic, harsh environments.
Also, there are environments that hinder direct piloting.\\
For example, in tunnel-like environments like sewers flight control has to be accurately
executed, because the tunnels can have small diameters. In addition,
the environment is not directly viewable and piloting relies on video material
that might be delayed and requires a given infrastructure. \\
Also, modern agriculture starts using drones for autonomous seed dropping or
as a low-cost remote sensing system. The variety of possible applications 
for autonomous UAVs is immense and is increasing til today.
As a consequence, also an increase of research could be observed - specializing 
on different aspects of this complex task.
One of these aspects is the accuracy of the drone: the ability to approach 
a defined waypoint and keep this position as accurately as possible if desired.
Also increasing the velocity of waypoint approaching has been researched.
For many tasks it is important to keep a steady flight in order to
use payloads like cameras effectively.
Therefore, high rotational velocities might not be preferred. 
With the use of Reinforcement Learning these aspects can be easily modelled
and used to construct an intelligent flight control.
This work focuses on a basic form of flight control: waypoint hovering.
\emph{Waypoint Hovering} can be described as approaching a waypoint as fast as 
possible and stay there independent of external influences that many harsh
environments show. It aims at comparing different approaches and showing
that RL can be used in order to create robust flight control.

\section{Related Work}
Most of the latest related work can be categorized into different sections,
because most autonomous UAV approaches split up the task into easier sub-tasks. 
\emph{Pathplanning} and \emph{Pathgeneration} deals with generating
an optimal path. A path can be described as a sequence of waypoints that models
a contiguous route threw 3D space. 
\emph{Sung et al.} (2018) \cite{kwak2018autonomous} proposes UAV flight control
based on a graph-based, generated path. The used hierarchical $A^*$ search
algorithms still relies on UAV flight data collected by pilots.
The generated path enables autonomous flight along shorter paths. 
\emph{Pathfollowing} is the task to reach each waypoint that is part of the path
with high accuracy. 
\emph{Nelson et al.} (2006) \cite{1657648} presents a Pathfollowing based
on vector fields for small UAVs that zeros the ground track heading error
and lateral following error asymptotically even in harsh environments. 
\emph{Indrawati et al.} (2015) 
\cite{indrawati2015waypoint}
used a Fuzzy Logic Controller for Waypoint Navigation that consists of 
a pitch control loop, a roll control loop and a vertical rate control loop.
Most of these approaches can be categorized as high level control approaches and
use the sensor data to generate or follow a path, but is not working directly
with the motors. They are based on the use of \emph{Attitude control} that controls 
the euler angles of the drone by directly sending motor signals.
The stability of the flight mostly relies on these attitude controllers.\\
\newline
\emph{Koch et al.} (2019) \cite{koch2019reinforcement} has shown that via 
Reinforcement Learning an intelligent agent can be learned that surpasses 
common attitude controller.
In addition, the authors have shown that this controller can be used as a part of 
a next generation flight control firmware called \emph{Neuroflight} \cite{koch2019neuroflight}.
Although \emph{Koch et al.} used its own simulation framework, mostly consisting
of \emph{Gazebo}, \emph{Penerati et al.} (2021) \cite{panerati2021learning} proposed
a simple Gym environment based on the Pybullet physics simulator \cite{coumans2021}.
This research will be an essential part of this work.\\
\newline
In Reinforcement Learning the used algorithms are of essential importance.
\emph{Schulman et al.} (2017) \cite{schulman2017proximal} proposed an onpolicy
algorithm called \emph{Proximal Policy Optimization} that defines a new surrogate objective.
\emph{Haarnoja et al.} (2018) \cite{haarnoja2018soft} proposed an offpolicy
algorithm called \emph{Soft Actor-Critic}. Both of these algorithms are used 
in this work.\\
Besides the algorithms, in RL a variety of research topics can be found.
With the use of \emph{Curriculum Learning} \cite{9207427} RL training can 
be significantly accelerated on multi-goal reaching tasks.
\emph{Florensa et al.} (2017) \cite{pmlr-v78-florensa17a} proposed a reversed
curriculum generation for RL with goal tasks, which makes the agent learn to reach a goal 
from a set of starting positions that increases in distance during the training process. 
Also, \emph{Baranes et al.} (2010) \cite{5651385} proposed a self adaptive goal generation, 
which allows a robot to learn its inverse kinematics.\\
The increase of performance is a main research topic. 
For example, Hierarchical RL \cite{botvinick2012hierarchical} aims at 
using a Hierarchy of models that individually solve sub problems.
When used in control tasks, explainability is an important research topic \cite{heuillet2021explainability},
because a stable control should be guaranteed.

