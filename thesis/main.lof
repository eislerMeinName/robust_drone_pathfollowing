\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces "X" and "+" configuration of a quadrocopter with the motors ($M_1 ... M_4$), the flight controller($FC$) and the rotational movement of the corresponding propeller (red clockwise, blue counterclockwise)\relax }}{4}{figure.caption.7}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of Roll, Pitch and Yaw rotational movement (from left to right) and the corresponding difference in the rotational speed $\omega _i$\relax }}{6}{figure.caption.12}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Sketch of an autonomous quadrocopter flight control system\relax }}{8}{figure.caption.17}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Concept onion of Artificial Intelligence \cite {NN}\relax }}{11}{figure.caption.24}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces The main learning strategies of machine learning including the related approaches like classification and regression under the Supervised Learning, clustering under the Unsupervised Learning and a sketch of the Reinforcement Learning concept\relax }}{12}{figure.caption.25}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces The basic concept of RL: the agent takes an action based on the observed state and receives a numerical reward at each time step\relax }}{13}{figure.caption.32}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Visualization of a simple MDP with 4 states ($s_0, s_1, s_2, s_3$) (blue nodes), 2 actions ($a_0, a_1$), state-action edges (green) and action-state edges (dark red) containing a tuple (possibility, reward)\relax }}{15}{figure.caption.35}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces The McCulloch-Pitts model of a single neuron uses a weighted sum with the inputs $x_i$ and weights $w_i$ and a non-linear activation function $g()$ in order to compute the output $z$ \cite {10.1063/1.1144830}\relax }}{20}{figure.caption.55}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces 8 different Activation Functions.\relax }}{21}{figure.caption.58}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Simple Feed-Forward Network with 3 neurons in the input layer, 2 hidden layers with 4 neurons and with 2 layers in the output layer\relax }}{22}{figure.caption.61}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces Overview of the different categories of RL algorithms with examples: PPO and DDPG Algorithm coloured because they were used in this work.\relax }}{23}{figure.caption.64}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Proposed Concept of autonomous flight control with the use of a intelligent agent implemented with the use of a NN. The output of the NN is denormalized in order to translate them to motor signals. With the use of the sensors and position estimation the input for the next control step are calculated.\relax }}{27}{figure.caption.70}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Concept of the implemented software: the different tools(red), scripts(violet) that are used in order to learn the intelligent Agent robust flight control with the use of RL.\relax }}{28}{figure.caption.71}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Visualization of the used reward function with the goal ($0,0,0.5$) and a colour scale: the distance dependent part (a) and the big penatalization, because the drone hit the ground (b)\relax }}{34}{figure.caption.85}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces shortened excerpt of the python implementation of the learning script\relax }}{46}{figure.caption.94}%
\addvspace {10\p@ }
\babel@toc {english}{}\relax 
\babel@toc {english}{}\relax 
\babel@toc {english}{}\relax 
\babel@toc {english}{}\relax 
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
